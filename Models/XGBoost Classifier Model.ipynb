{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Let's try to create a model that identifies fake and real news.</h1><br>\n",
    "the dataset will be https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to train a separate model for fake news and real news and then combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   \n",
       "0   Donald Trump Sends Out Embarrassing New Year’...  \\\n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject   \n",
       "0  Donald Trump just couldn t wish all Americans ...    News  \\\n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import spacy\n",
    "\n",
    "df_fake=pd.read_csv('..//Datasets/Fake_news.csv')\n",
    "df_real=pd.read_csv('..//Datasets/Real_news.csv')\n",
    "\n",
    "df_fake.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing .<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to remove links and special chars, also removing extra whitespaces.\n",
    "def clean_text(text):  \n",
    "    # remove links by substituting target words with nothing\n",
    "    text=re.sub(r'http\\S+|www\\S+|https\\S+', '',text)\n",
    "    # remove special chars\n",
    "    text=re.sub(r'[^a-zA-ZÀ-ú\\s]','',text)\n",
    "    # tokenization\n",
    "    tokens=text.split()\n",
    "    # remove extra whitespace\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    cleaned_text=' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "def possifier(text_df):\n",
    "    pos_tags_list = []\n",
    "    for text in text_df:\n",
    "        doc = nlp(text)\n",
    "        pos_tags = [(token.pos_) for token in doc]\n",
    "        pos_tags_list.append(pos_tags)\n",
    "    return pos_tags_list\n",
    "\n",
    "# adding pos_tag counts might help the model further\n",
    "def process_pos_tags(tags):\n",
    "    pos_counts = {}\n",
    "    for tag in tags:\n",
    "        if tag in pos_counts:\n",
    "            pos_counts[tag] += 1\n",
    "        else:\n",
    "            pos_counts[tag] = 1\n",
    "    return pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'D': 21, 'o': 142, 'n': 140, 'a': 182, 'l': 9...\n",
       "1    {'H': 5, 'o': 125, 'u': 45, 's': 92, 'e': 173,...\n",
       "2    {'O': 7, 'n': 137, ' ': 599, 'F': 9, 'r': 173,...\n",
       "3    {'O': 8, 'n': 97, ' ': 474, 'C': 9, 'h': 104, ...\n",
       "4    {'P': 7, 'o': 155, 'p': 36, 'e': 257, ' ': 433...\n",
       "Name: pos_counts_per_row, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_fake['text']=[clean_text(text) for text in df_fake['text']]\n",
    "#df_real['text']=[clean_text(text) for text in df_real['text']]\n",
    "df_fake['pos counts'] = df_fake['text'].apply(process_pos_tags)\n",
    "df_fake.pos_counts_per_row.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I have no target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# training data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[39m# Target is the sentiment so y=sentiment\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "X=df['message']\n",
    "\n",
    "# Target is the sentiment so y=sentiment\n",
    "y=df['sentiment']\n",
    "\n",
    "cv=CountVectorizer()\n",
    "X=cv.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "le=LabelEncoder()\n",
    "y_test_encoded=le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:0.97773\n",
      "[1]\tvalidation_0-mlogloss:0.88978\n",
      "[2]\tvalidation_0-mlogloss:0.83131\n",
      "[3]\tvalidation_0-mlogloss:0.78910\n",
      "[4]\tvalidation_0-mlogloss:0.74464\n",
      "[5]\tvalidation_0-mlogloss:0.72004\n",
      "[6]\tvalidation_0-mlogloss:0.69919\n",
      "[7]\tvalidation_0-mlogloss:0.67303\n",
      "[8]\tvalidation_0-mlogloss:0.65912\n",
      "[9]\tvalidation_0-mlogloss:0.64356\n",
      "[10]\tvalidation_0-mlogloss:0.63320\n",
      "[11]\tvalidation_0-mlogloss:0.61390\n",
      "[12]\tvalidation_0-mlogloss:0.60269\n",
      "[13]\tvalidation_0-mlogloss:0.59335\n",
      "[14]\tvalidation_0-mlogloss:0.58258\n",
      "[15]\tvalidation_0-mlogloss:0.57563\n",
      "[16]\tvalidation_0-mlogloss:0.56660\n",
      "[17]\tvalidation_0-mlogloss:0.55860\n",
      "[18]\tvalidation_0-mlogloss:0.55364\n",
      "[19]\tvalidation_0-mlogloss:0.54344\n",
      "[20]\tvalidation_0-mlogloss:0.53658\n",
      "[21]\tvalidation_0-mlogloss:0.53227\n",
      "[22]\tvalidation_0-mlogloss:0.52762\n",
      "[23]\tvalidation_0-mlogloss:0.52534\n",
      "[24]\tvalidation_0-mlogloss:0.52158\n",
      "[25]\tvalidation_0-mlogloss:0.51799\n",
      "[26]\tvalidation_0-mlogloss:0.51574\n",
      "[27]\tvalidation_0-mlogloss:0.51042\n",
      "[28]\tvalidation_0-mlogloss:0.50683\n",
      "[29]\tvalidation_0-mlogloss:0.50572\n",
      "[30]\tvalidation_0-mlogloss:0.50311\n",
      "[31]\tvalidation_0-mlogloss:0.49986\n",
      "[32]\tvalidation_0-mlogloss:0.49910\n",
      "[33]\tvalidation_0-mlogloss:0.49647\n",
      "[34]\tvalidation_0-mlogloss:0.49635\n",
      "[35]\tvalidation_0-mlogloss:0.49229\n",
      "[36]\tvalidation_0-mlogloss:0.48847\n",
      "[37]\tvalidation_0-mlogloss:0.48715\n",
      "[38]\tvalidation_0-mlogloss:0.48455\n",
      "[39]\tvalidation_0-mlogloss:0.48348\n",
      "[40]\tvalidation_0-mlogloss:0.48334\n",
      "[41]\tvalidation_0-mlogloss:0.48297\n",
      "[42]\tvalidation_0-mlogloss:0.48204\n",
      "[43]\tvalidation_0-mlogloss:0.48231\n",
      "[44]\tvalidation_0-mlogloss:0.48323\n",
      "[45]\tvalidation_0-mlogloss:0.48121\n",
      "[46]\tvalidation_0-mlogloss:0.47939\n",
      "[47]\tvalidation_0-mlogloss:0.48015\n",
      "[48]\tvalidation_0-mlogloss:0.47971\n",
      "[49]\tvalidation_0-mlogloss:0.47870\n",
      "[50]\tvalidation_0-mlogloss:0.47772\n",
      "[51]\tvalidation_0-mlogloss:0.47895\n",
      "[52]\tvalidation_0-mlogloss:0.47878\n",
      "[53]\tvalidation_0-mlogloss:0.47763\n",
      "[54]\tvalidation_0-mlogloss:0.47713\n",
      "[55]\tvalidation_0-mlogloss:0.48012\n",
      "[56]\tvalidation_0-mlogloss:0.47985\n",
      "[57]\tvalidation_0-mlogloss:0.47907\n",
      "[58]\tvalidation_0-mlogloss:0.48060\n",
      "[59]\tvalidation_0-mlogloss:0.48171\n"
     ]
    }
   ],
   "source": [
    "# Let's try XGBoost\n",
    "# Unlike MultinomialNB, XGBClassifier demands all data to be numerical.\n",
    "# So let's do that by using what we used previously, the label encoder\n",
    "y_train_encoded=le.fit_transform(y_train)\n",
    "\n",
    "# Converting X_train and X_test to dense arrays in order for them to work with XGBoost\n",
    "X_train_dense=X_train.toarray()\n",
    "X_test_dense=X_test.toarray()\n",
    "\n",
    "# one hot encoding categorical values\n",
    "encoder=OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(X_train_dense)\n",
    "X_train_encoded=encoder.transform(X_train_dense)\n",
    "X_test_encoded=encoder.transform(X_test_dense)\n",
    "\n",
    "# Initiating regressor...I mean classifier, so far n_estimators=300 and learning_rate=0.3 seem the best\n",
    "xgb=XGBClassifier(n_estimators=1000,learning_rate=0.25,early_stopping_rounds=5)\n",
    "\n",
    "# Fitting model, in order to use early_stopping_rounds we need to add an eval_set to the fitting part.\n",
    "xgb.fit(X_train_encoded,y_train_encoded, eval_set=[(X_test_encoded, y_test_encoded)], verbose=True)\n",
    "\n",
    "# Predicting\n",
    "pred_xgb_split=xgb.predict(X_test_encoded)\n",
    "\n",
    "# Convert numerical predictions back to categorical labels\n",
    "y_pred_labels = le.inverse_transform(pred_xgb_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparams seem to be n_estimators=1000,learning_rate=0.25 where early_stopping_rounds=5. Therefore i shall train an army of forces to defeat Sauron. \n",
    "xgb_full=XGBClassifier(n_estimators=1000,learning_rate=0.25)\n",
    "# The full data X must be encoded since it is not categorical, while the target data y is.\n",
    "encoder.fit(X.toarray())\n",
    "X_encoded = encoder.transform(X.toarray())\n",
    "y_encoded=le.fit_transform(y)\n",
    "xgb_full.fit(X_encoded,y_encoded)\n",
    "\n",
    "# Predicting\n",
    "pred_xgb_full = xgb_full.predict(X_encoded)\n",
    "\n",
    "# Convert numerical predictions back to categorical labels\n",
    "y_pred_labels_full = le.inverse_transform(pred_xgb_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the split model: 0.8125\n",
      "Mean Squared Error for the split model: 0.5965909090909091\n",
      "Mean Absolute Error for the split model: 0.32386363636363635\n",
      "Accuracy for the full model: 0.9691780821917808\n",
      "Mean Squared Error for the full model: 0.08732876712328767\n",
      "Mean Absolute Error for the full model: 0.04965753424657534\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "mse = mean_squared_error(y_test_encoded, pred_xgb_split)\n",
    "mae = mean_absolute_error(y_test_encoded, pred_xgb_split)\n",
    "\n",
    "accuracyf = accuracy_score(y, y_pred_labels_full)\n",
    "msef = mean_squared_error(y_encoded, pred_xgb_full)\n",
    "maef = mean_absolute_error(y_encoded, pred_xgb_full)\n",
    "\n",
    "print(f'Accuracy for the split model: {accuracy}')\n",
    "print(f'Mean Squared Error for the split model: {mse}')\n",
    "print (f'Mean Absolute Error for the split model: {mae}')\n",
    "print(f'Accuracy for the full model: {accuracyf}')\n",
    "print(f'Mean Squared Error for the full model: {msef}')\n",
    "print (f'Mean Absolute Error for the full model: {maef}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
